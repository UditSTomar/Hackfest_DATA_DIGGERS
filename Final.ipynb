{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L4Wyc_9laSQW"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "def load_data(path):\n",
    "    input_file = os.path.join(path)\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        data = f.read()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N1tf9QOWQile"
   },
   "source": [
    "## Getting the data paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uaEhm0d4aSSd"
   },
   "outputs": [],
   "source": [
    "source_path = 'C:/Users/ROHIT/Desktop/Hackfest/Eng2.csv'\n",
    "target_path = 'C:/Users/ROHIT/Desktop/Hackfest/Bin2.csv'\n",
    "source_text = load_data(source_path)\n",
    "target_text = load_data(target_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bx9g-mR2Qt-Q"
   },
   "source": [
    "## Making tables mapping words in sentences of text to their unique indices and vice versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z0SCBrZ1aSU1"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g379Jgb5aSW1"
   },
   "outputs": [],
   "source": [
    "CODES = {'<PAD>': 0, '<EOS>': 1, '<UNK>': 2, '<GO>': 3 }\n",
    "\n",
    "def create_lookup_tables(text):\n",
    "    # make a list of unique words\n",
    "    vocab = set(text.split())\n",
    "\n",
    "    # (1)\n",
    "    # starts with the special tokens\n",
    "    vocab_to_int = copy.copy(CODES)\n",
    "\n",
    "    # the index (v_i) will starts from 4 (the 2nd arg in enumerate() specifies the starting index)\n",
    "    # since vocab_to_int already contains special tokens\n",
    "    for v_i, v in enumerate(vocab, len(CODES)):\n",
    "        vocab_to_int[v] = v_i\n",
    "\n",
    "    # (2)\n",
    "    int_to_vocab = {v_i: v for v, v_i in vocab_to_int.items()}\n",
    "\n",
    "    return vocab_to_int, int_to_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Co6jLXnoaSZG"
   },
   "outputs": [],
   "source": [
    "def text_to_ids(source_text, target_text, source_vocab_to_int, target_vocab_to_int):\n",
    "    \"\"\"\n",
    "        1st, 2nd args: raw string text to be converted\n",
    "        3rd, 4th args: lookup tables for 1st and 2nd args respectively\n",
    "    \n",
    "        return: A tuple of lists (source_id_text, target_id_text) converted\n",
    "    \"\"\"\n",
    "    # empty list of converted sentences\n",
    "    source_text_id = []\n",
    "    target_text_id = []\n",
    "    \n",
    "    # make a list of sentences (extraction)\n",
    "    source_sentences = source_text.split(\"\\n\")\n",
    "    target_sentences = target_text.split(\"\\n\")\n",
    "    \n",
    "    max_source_sentence_length = max([len(sentence.split(\" \")) for sentence in source_sentences])\n",
    "    max_target_sentence_length = max([len(sentence.split(\" \")) for sentence in target_sentences])\n",
    "    \n",
    "    # iterating through each sentences (# of sentences in source&target is the same)\n",
    "    for i in range(len(source_sentences)):\n",
    "        # extract sentences one by one\n",
    "        source_sentence = source_sentences[i]\n",
    "        target_sentence = target_sentences[i]\n",
    "        \n",
    "        # make a list of tokens/words (extraction) from the chosen sentence\n",
    "        source_tokens = source_sentence.split(\" \")\n",
    "        target_tokens = target_sentence.split(\" \")\n",
    "        \n",
    "        # empty list of converted words to index in the chosen sentence\n",
    "        source_token_id = []\n",
    "        target_token_id = []\n",
    "        \n",
    "        for index, token in enumerate(source_tokens):\n",
    "            if (token != \"\"):\n",
    "                source_token_id.append(source_vocab_to_int[token])\n",
    "        \n",
    "        for index, token in enumerate(target_tokens):\n",
    "            if (token != \"\"):\n",
    "                target_token_id.append(target_vocab_to_int[token])\n",
    "                \n",
    "        # put <EOS> token at the end of the chosen target sentence\n",
    "        # this token suggests when to stop creating a sequence\n",
    "        target_token_id.append(target_vocab_to_int['<EOS>'])\n",
    "            \n",
    "        # add each converted sentences in the final list\n",
    "        source_text_id.append(source_token_id)\n",
    "        target_text_id.append(target_token_id)\n",
    "    \n",
    "    return source_text_id, target_text_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fwv4ayWpaSat"
   },
   "outputs": [],
   "source": [
    "def preprocess_and_save_data(source_path, target_path, text_to_ids):\n",
    "    # Preprocess\n",
    "    \n",
    "    # load original data (English, French)\n",
    "    source_text = load_data(source_path)\n",
    "    target_text = load_data(target_path)\n",
    "\n",
    "    # to the lower case\n",
    "    source_text = source_text.lower()\n",
    "    target_text = target_text.lower()\n",
    "\n",
    "    # create lookup tables for English and French data\n",
    "    source_vocab_to_int, source_int_to_vocab = create_lookup_tables(source_text)\n",
    "    target_vocab_to_int, target_int_to_vocab = create_lookup_tables(target_text)\n",
    "\n",
    "    # create list of sentences whose words are represented in index\n",
    "    source_text, target_text = text_to_ids(source_text, target_text, source_vocab_to_int, target_vocab_to_int)\n",
    "\n",
    "    # Save data for later use\n",
    "    pickle.dump((\n",
    "        (source_text, target_text),\n",
    "        (source_vocab_to_int, target_vocab_to_int),\n",
    "        (source_int_to_vocab, target_int_to_vocab)), open('preprocess.p', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Df2QD7mkRvi_"
   },
   "source": [
    "# Defining the model: LSTM-Encoder Decoder architecture based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_qIeRowyaScd"
   },
   "outputs": [],
   "source": [
    "preprocess_and_save_data(source_path, target_path, text_to_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i2b4KDl0aSeV"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def load_preprocess():\n",
    "    with open('preprocess.p', mode='rb') as in_file:\n",
    "        return pickle.load(in_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pkl69_vNaSgO"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "(source_int_text, target_int_text), (source_vocab_to_int, target_vocab_to_int), _ = load_preprocess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PRTZd7NmSJbF"
   },
   "source": [
    "## Inputs for encoder and decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p87rBeb5aSin"
   },
   "outputs": [],
   "source": [
    "def enc_dec_model_inputs():\n",
    "    inputs = tf.placeholder(tf.int32, [None, None], name='input')\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name='targets') \n",
    "    \n",
    "    target_sequence_length = tf.placeholder(tf.int32, [None], name='target_sequence_length')\n",
    "    max_target_len = tf.reduce_max(target_sequence_length)    \n",
    "    \n",
    "    return inputs, targets, target_sequence_length, max_target_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kjJbeUtWaSlQ"
   },
   "outputs": [],
   "source": [
    "def hyperparam_inputs():\n",
    "    lr_rate = tf.placeholder(tf.float32, name='lr_rate')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    return lr_rate, keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nuoQwJMfaSnq"
   },
   "outputs": [],
   "source": [
    "def process_decoder_input(target_data, target_vocab_to_int, batch_size):\n",
    "    \n",
    "    go_id = target_vocab_to_int['<GO>']\n",
    "    \n",
    "    after_slice = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n",
    "    after_concat = tf.concat( [tf.fill([batch_size, 1], go_id), after_slice], 1)\n",
    "    \n",
    "    return after_concat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SujPcb5VSWR4"
   },
   "source": [
    "## Encoding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VEuWb_KAaSpf"
   },
   "outputs": [],
   "source": [
    "def encoding_layer(rnn_inputs, rnn_size, num_layers, keep_prob, \n",
    "                   source_vocab_size, \n",
    "                   encoding_embedding_size):\n",
    "    \n",
    "    embed = tf.contrib.layers.embed_sequence(rnn_inputs, \n",
    "                                             vocab_size=source_vocab_size, \n",
    "                                             embed_dim=encoding_embedding_size)\n",
    "    \n",
    "    stacked_cells = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.DropoutWrapper(tf.contrib.rnn.LSTMCell(rnn_size), keep_prob) for _ in range(num_layers)])\n",
    "    \n",
    "    outputs, state = tf.nn.dynamic_rnn(stacked_cells, \n",
    "                                       embed, \n",
    "                                       dtype=tf.float32)\n",
    "    return outputs, state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kncS6vE_Ski4"
   },
   "source": [
    "## Decoding layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bE7XARDbSncH"
   },
   "source": [
    "### Train layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UoP27cvNaSu9"
   },
   "outputs": [],
   "source": [
    "def decoding_layer_train(encoder_state, dec_cell, dec_embed_input, \n",
    "                         target_sequence_length, max_summary_length, \n",
    "                         output_layer, keep_prob):\n",
    "    \n",
    "    dec_cell = tf.contrib.rnn.DropoutWrapper(dec_cell, \n",
    "                                             output_keep_prob=keep_prob)\n",
    "    \n",
    "    # for only input layer\n",
    "    helper = tf.contrib.seq2seq.TrainingHelper(dec_embed_input, \n",
    "                                               target_sequence_length)\n",
    "    \n",
    "    decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell, \n",
    "                                              helper, \n",
    "                                              encoder_state, \n",
    "                                              output_layer)\n",
    "\n",
    "    # unrolling the decoder layer\n",
    "    outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder, \n",
    "                                                      impute_finished=True, \n",
    "                                                      maximum_iterations=max_summary_length)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nHpDLnM3SeKP"
   },
   "source": [
    "### Decoding Layer for inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iIhijfHNSg6m"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QTu4MyMYa4_t"
   },
   "outputs": [],
   "source": [
    "def decoding_layer_infer(encoder_state, dec_cell, dec_embeddings, start_of_sequence_id,\n",
    "                         end_of_sequence_id, max_target_sequence_length,\n",
    "                         vocab_size, output_layer, batch_size, keep_prob):\n",
    "    \n",
    "    dec_cell = tf.contrib.rnn.DropoutWrapper(dec_cell, \n",
    "                                             output_keep_prob=keep_prob)\n",
    "    \n",
    "    helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(dec_embeddings, \n",
    "                                                      tf.fill([batch_size], start_of_sequence_id), \n",
    "                                                      end_of_sequence_id)\n",
    "    \n",
    "    decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell, \n",
    "                                              helper, \n",
    "                                              encoder_state, \n",
    "                                              output_layer)\n",
    "    \n",
    "    outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder, \n",
    "                                                      impute_finished=True, \n",
    "                                                      maximum_iterations=max_target_sequence_length)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tkF-97EzSzHu"
   },
   "source": [
    "### Combined decoding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w7mvCy_Ka5CB"
   },
   "outputs": [],
   "source": [
    "def decoding_layer(dec_input, encoder_state,\n",
    "                   target_sequence_length, max_target_sequence_length,\n",
    "                   rnn_size,\n",
    "                   num_layers, target_vocab_to_int, target_vocab_size,\n",
    "                   batch_size, keep_prob, decoding_embedding_size):\n",
    "    target_vocab_size = len(target_vocab_to_int)\n",
    "    dec_embeddings = tf.Variable(tf.random_uniform([target_vocab_size, decoding_embedding_size]))\n",
    "    dec_embed_input = tf.nn.embedding_lookup(dec_embeddings, dec_input)\n",
    "    \n",
    "    cells = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.LSTMCell(rnn_size) for _ in range(num_layers)])\n",
    "    \n",
    "    with tf.variable_scope(\"decode\"):\n",
    "        output_layer = tf.layers.Dense(target_vocab_size)\n",
    "        train_output = decoding_layer_train(encoder_state, \n",
    "                                            cells, \n",
    "                                            dec_embed_input, \n",
    "                                            target_sequence_length, \n",
    "                                            max_target_sequence_length, \n",
    "                                            output_layer, \n",
    "                                            keep_prob)\n",
    "\n",
    "    with tf.variable_scope(\"decode\", reuse=True):\n",
    "        infer_output = decoding_layer_infer(encoder_state, \n",
    "                                            cells, \n",
    "                                            dec_embeddings, \n",
    "                                            target_vocab_to_int['<GO>'], \n",
    "                                            target_vocab_to_int['<EOS>'], \n",
    "                                            max_target_sequence_length, \n",
    "                                            target_vocab_size, \n",
    "                                            output_layer,\n",
    "                                            batch_size,\n",
    "                                            keep_prob)\n",
    "\n",
    "    return (train_output, infer_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "faea1MtCTFeh"
   },
   "source": [
    "### Final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RzP4QMBTa5Ef"
   },
   "outputs": [],
   "source": [
    "def seq2seq_model(input_data, target_data, keep_prob, batch_size,\n",
    "                  target_sequence_length,\n",
    "                  max_target_sentence_length,\n",
    "                  source_vocab_size, target_vocab_size,\n",
    "                  enc_embedding_size, dec_embedding_size,\n",
    "                  rnn_size, num_layers, target_vocab_to_int):\n",
    "    \n",
    "    enc_outputs, enc_states = encoding_layer(input_data, \n",
    "                                             rnn_size, \n",
    "                                             num_layers, \n",
    "                                             keep_prob, \n",
    "                                             source_vocab_size, \n",
    "                                             enc_embedding_size)\n",
    "    \n",
    "    dec_input = process_decoder_input(target_data, \n",
    "                                      target_vocab_to_int, \n",
    "                                      batch_size)\n",
    "    \n",
    "    train_output, infer_output = decoding_layer(dec_input,\n",
    "                                               enc_states, \n",
    "                                               target_sequence_length, \n",
    "                                               max_target_sentence_length,\n",
    "                                               rnn_size,\n",
    "                                              num_layers,\n",
    "                                              target_vocab_to_int,\n",
    "                                              target_vocab_size,\n",
    "                                              batch_size,\n",
    "                                              keep_prob,\n",
    "                                              dec_embedding_size)\n",
    "    \n",
    "    return train_output, infer_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HD-kvlQBU2d4"
   },
   "source": [
    "## Defining model hyperparameters, loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GijJwi10a5Gx"
   },
   "outputs": [],
   "source": [
    "display_step = 2\n",
    "\n",
    "epochs = 300\n",
    "batch_size = 15\n",
    "\n",
    "rnn_size = 80\n",
    "num_layers = 5\n",
    "\n",
    "encoding_embedding_size = 20\n",
    "decoding_embedding_size = 20\n",
    "\n",
    "learning_rate = 0.001\n",
    "keep_probability = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Yl2N2dQDa5JH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-13-340f8791e33f>:9: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-13-340f8791e33f>:9: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-13-340f8791e33f>:13: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py:1259: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "save_path = 'C:/Users/ROHIT/Desktop/Hackfest/3/model/checkpoints/dev'\n",
    "(source_int_text, target_int_text), (source_vocab_to_int, target_vocab_to_int), _ = load_preprocess()\n",
    "max_target_sentence_length = max([len(sentence) for sentence in source_int_text])\n",
    "\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    input_data, targets, target_sequence_length, max_target_sequence_length = enc_dec_model_inputs()\n",
    "    lr, keep_prob = hyperparam_inputs()\n",
    "    \n",
    "    train_logits, inference_logits = seq2seq_model(tf.reverse(input_data, [-1]),\n",
    "                                                   targets,\n",
    "                                                   keep_prob,\n",
    "                                                   batch_size,\n",
    "                                                   target_sequence_length,\n",
    "                                                   max_target_sequence_length,\n",
    "                                                   len(source_vocab_to_int),\n",
    "                                                   len(target_vocab_to_int),\n",
    "                                                   encoding_embedding_size,\n",
    "                                                   decoding_embedding_size,\n",
    "                                                   rnn_size,\n",
    "                                                   num_layers,\n",
    "                                                   target_vocab_to_int)\n",
    "    \n",
    "    training_logits = tf.identity(train_logits.rnn_output, name='logits')\n",
    "    inference_logits = tf.identity(inference_logits.sample_id, name='predictions')\n",
    "\n",
    "    \n",
    "    masks = tf.sequence_mask(target_sequence_length, max_target_sequence_length, dtype=tf.float32, name='masks')\n",
    "\n",
    "    with tf.name_scope(\"optimization\"):\n",
    "        # Loss function - weighted softmax cross entropy\n",
    "        cost = tf.contrib.seq2seq.sequence_loss(\n",
    "            training_logits,\n",
    "            targets,\n",
    "            masks)\n",
    "\n",
    "        # Optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "        # Gradient Clipping\n",
    "        gradients = optimizer.compute_gradients(cost)\n",
    "        capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
    "        train_op = optimizer.apply_gradients(capped_gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IDc5nDlwVazX"
   },
   "source": [
    "## For generating batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N23ePGkabKGz"
   },
   "outputs": [],
   "source": [
    "def pad_sentence_batch(sentence_batch, pad_int):\n",
    "    \"\"\"Pad sentences with <PAD> so that each sentence of a batch has the same length\"\"\"\n",
    "    max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
    "    return [sentence + [pad_int] * (max_sentence - len(sentence)) for sentence in sentence_batch]\n",
    "\n",
    "\n",
    "def get_batches(sources, targets, batch_size, source_pad_int, target_pad_int):\n",
    "    \"\"\"Batch targets, sources, and the lengths of their sentences together\"\"\"\n",
    "    for batch_i in range(0, len(sources)//batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "\n",
    "        # Slice the right amount for the batch\n",
    "        sources_batch = sources[start_i:start_i + batch_size]\n",
    "        targets_batch = targets[start_i:start_i + batch_size]\n",
    "\n",
    "        # Pad\n",
    "        pad_sources_batch = np.array(pad_sentence_batch(sources_batch, source_pad_int))\n",
    "        pad_targets_batch = np.array(pad_sentence_batch(targets_batch, target_pad_int))\n",
    "\n",
    "        # Need the lengths for the _lengths parameters\n",
    "        pad_targets_lengths = []\n",
    "        for target in pad_targets_batch:\n",
    "            pad_targets_lengths.append(len(target))\n",
    "\n",
    "        pad_source_lengths = []\n",
    "        for source in pad_sources_batch:\n",
    "            pad_source_lengths.append(len(source))\n",
    "\n",
    "        yield pad_sources_batch, pad_targets_batch, pad_source_lengths, pad_targets_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P_4zuJYPUfHn"
   },
   "source": [
    "## Now, to train model and get accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "xNoqGtQXbKJi",
    "outputId": "3bf881df-a411-4fdb-ebd6-8f50f2a34a07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch    2/4 - Train Accuracy: 0.3500, Validation Accuracy: 0.3333, Loss: 3.5615\n",
      "Epoch   1 Batch    2/4 - Train Accuracy: 0.3500, Validation Accuracy: 0.3333, Loss: 3.4937\n",
      "Epoch   2 Batch    2/4 - Train Accuracy: 0.3500, Validation Accuracy: 0.3333, Loss: 3.2982\n",
      "Epoch   3 Batch    2/4 - Train Accuracy: 0.3500, Validation Accuracy: 0.3333, Loss: 2.8041\n",
      "Epoch   4 Batch    2/4 - Train Accuracy: 0.3500, Validation Accuracy: 0.3333, Loss: 2.5341\n",
      "Epoch   5 Batch    2/4 - Train Accuracy: 0.3500, Validation Accuracy: 0.3333, Loss: 2.5203\n",
      "Epoch   6 Batch    2/4 - Train Accuracy: 0.3583, Validation Accuracy: 0.3333, Loss: 2.5281\n",
      "Epoch   7 Batch    2/4 - Train Accuracy: 0.3583, Validation Accuracy: 0.3600, Loss: 2.4858\n",
      "Epoch   8 Batch    2/4 - Train Accuracy: 0.3917, Validation Accuracy: 0.3600, Loss: 2.4467\n",
      "Epoch   9 Batch    2/4 - Train Accuracy: 0.4000, Validation Accuracy: 0.3600, Loss: 2.4657\n",
      "Epoch  10 Batch    2/4 - Train Accuracy: 0.4000, Validation Accuracy: 0.3600, Loss: 2.4132\n",
      "Epoch  11 Batch    2/4 - Train Accuracy: 0.4000, Validation Accuracy: 0.3600, Loss: 2.3944\n",
      "Epoch  12 Batch    2/4 - Train Accuracy: 0.4000, Validation Accuracy: 0.3600, Loss: 2.3781\n",
      "Epoch  13 Batch    2/4 - Train Accuracy: 0.4000, Validation Accuracy: 0.3600, Loss: 2.3586\n",
      "Epoch  14 Batch    2/4 - Train Accuracy: 0.4000, Validation Accuracy: 0.3600, Loss: 2.3177\n",
      "Epoch  15 Batch    2/4 - Train Accuracy: 0.4083, Validation Accuracy: 0.3667, Loss: 2.2736\n",
      "Epoch  16 Batch    2/4 - Train Accuracy: 0.4000, Validation Accuracy: 0.3667, Loss: 2.2700\n",
      "Epoch  17 Batch    2/4 - Train Accuracy: 0.4000, Validation Accuracy: 0.3667, Loss: 2.3235\n",
      "Epoch  18 Batch    2/4 - Train Accuracy: 0.4083, Validation Accuracy: 0.3667, Loss: 2.2132\n",
      "Epoch  19 Batch    2/4 - Train Accuracy: 0.4000, Validation Accuracy: 0.3667, Loss: 2.1519\n",
      "Epoch  20 Batch    2/4 - Train Accuracy: 0.4000, Validation Accuracy: 0.3733, Loss: 2.1336\n",
      "Epoch  21 Batch    2/4 - Train Accuracy: 0.4000, Validation Accuracy: 0.3667, Loss: 2.1025\n",
      "Epoch  22 Batch    2/4 - Train Accuracy: 0.3917, Validation Accuracy: 0.3667, Loss: 2.1133\n",
      "Epoch  23 Batch    2/4 - Train Accuracy: 0.3917, Validation Accuracy: 0.3667, Loss: 2.0803\n",
      "Epoch  24 Batch    2/4 - Train Accuracy: 0.3917, Validation Accuracy: 0.3667, Loss: 2.0490\n",
      "Epoch  25 Batch    2/4 - Train Accuracy: 0.3917, Validation Accuracy: 0.3667, Loss: 2.0689\n",
      "Epoch  26 Batch    2/4 - Train Accuracy: 0.3917, Validation Accuracy: 0.3667, Loss: 2.0451\n",
      "Epoch  27 Batch    2/4 - Train Accuracy: 0.4083, Validation Accuracy: 0.3667, Loss: 2.0218\n",
      "Epoch  28 Batch    2/4 - Train Accuracy: 0.4167, Validation Accuracy: 0.3667, Loss: 1.9508\n",
      "Epoch  29 Batch    2/4 - Train Accuracy: 0.4083, Validation Accuracy: 0.3667, Loss: 2.0008\n",
      "Epoch  30 Batch    2/4 - Train Accuracy: 0.4167, Validation Accuracy: 0.3667, Loss: 1.9610\n",
      "Epoch  31 Batch    2/4 - Train Accuracy: 0.4083, Validation Accuracy: 0.3800, Loss: 2.0060\n",
      "Epoch  32 Batch    2/4 - Train Accuracy: 0.4167, Validation Accuracy: 0.3800, Loss: 1.9930\n",
      "Epoch  33 Batch    2/4 - Train Accuracy: 0.4250, Validation Accuracy: 0.3867, Loss: 1.9488\n",
      "Epoch  34 Batch    2/4 - Train Accuracy: 0.4250, Validation Accuracy: 0.3600, Loss: 1.9024\n",
      "Epoch  35 Batch    2/4 - Train Accuracy: 0.4333, Validation Accuracy: 0.3733, Loss: 1.9101\n",
      "Epoch  36 Batch    2/4 - Train Accuracy: 0.4333, Validation Accuracy: 0.3800, Loss: 1.8650\n",
      "Epoch  37 Batch    2/4 - Train Accuracy: 0.4250, Validation Accuracy: 0.3733, Loss: 1.8432\n",
      "Epoch  38 Batch    2/4 - Train Accuracy: 0.4250, Validation Accuracy: 0.3800, Loss: 1.8319\n",
      "Epoch  39 Batch    2/4 - Train Accuracy: 0.4250, Validation Accuracy: 0.4000, Loss: 1.8305\n",
      "Epoch  40 Batch    2/4 - Train Accuracy: 0.4417, Validation Accuracy: 0.4000, Loss: 1.8317\n",
      "Epoch  41 Batch    2/4 - Train Accuracy: 0.4250, Validation Accuracy: 0.4067, Loss: 1.8432\n",
      "Epoch  42 Batch    2/4 - Train Accuracy: 0.4250, Validation Accuracy: 0.4133, Loss: 1.7519\n",
      "Epoch  43 Batch    2/4 - Train Accuracy: 0.4333, Validation Accuracy: 0.4200, Loss: 1.7985\n",
      "Epoch  44 Batch    2/4 - Train Accuracy: 0.4333, Validation Accuracy: 0.4333, Loss: 1.7456\n",
      "Epoch  45 Batch    2/4 - Train Accuracy: 0.4250, Validation Accuracy: 0.4067, Loss: 1.7597\n",
      "Epoch  46 Batch    2/4 - Train Accuracy: 0.4667, Validation Accuracy: 0.4200, Loss: 1.8125\n",
      "Epoch  47 Batch    2/4 - Train Accuracy: 0.4750, Validation Accuracy: 0.4067, Loss: 1.7847\n",
      "Epoch  48 Batch    2/4 - Train Accuracy: 0.4500, Validation Accuracy: 0.4067, Loss: 1.7245\n",
      "Epoch  49 Batch    2/4 - Train Accuracy: 0.4750, Validation Accuracy: 0.4400, Loss: 1.7292\n",
      "Epoch  50 Batch    2/4 - Train Accuracy: 0.4333, Validation Accuracy: 0.3867, Loss: 1.7769\n",
      "Epoch  51 Batch    2/4 - Train Accuracy: 0.4333, Validation Accuracy: 0.4000, Loss: 1.7368\n",
      "Epoch  52 Batch    2/4 - Train Accuracy: 0.4333, Validation Accuracy: 0.4000, Loss: 1.6209\n",
      "Epoch  53 Batch    2/4 - Train Accuracy: 0.4333, Validation Accuracy: 0.4000, Loss: 1.7422\n",
      "Epoch  54 Batch    2/4 - Train Accuracy: 0.4750, Validation Accuracy: 0.4067, Loss: 1.8002\n",
      "Epoch  55 Batch    2/4 - Train Accuracy: 0.4833, Validation Accuracy: 0.4067, Loss: 1.6379\n",
      "Epoch  56 Batch    2/4 - Train Accuracy: 0.4500, Validation Accuracy: 0.4133, Loss: 1.7294\n",
      "Epoch  57 Batch    2/4 - Train Accuracy: 0.4500, Validation Accuracy: 0.4200, Loss: 1.7067\n",
      "Epoch  58 Batch    2/4 - Train Accuracy: 0.4333, Validation Accuracy: 0.4133, Loss: 1.7439\n",
      "Epoch  59 Batch    2/4 - Train Accuracy: 0.4500, Validation Accuracy: 0.4200, Loss: 1.7272\n",
      "Epoch  60 Batch    2/4 - Train Accuracy: 0.4333, Validation Accuracy: 0.4267, Loss: 1.6262\n",
      "Epoch  61 Batch    2/4 - Train Accuracy: 0.4667, Validation Accuracy: 0.4333, Loss: 1.5625\n",
      "Epoch  62 Batch    2/4 - Train Accuracy: 0.4583, Validation Accuracy: 0.4333, Loss: 1.6995\n",
      "Epoch  63 Batch    2/4 - Train Accuracy: 0.4333, Validation Accuracy: 0.4133, Loss: 1.6120\n",
      "Epoch  64 Batch    2/4 - Train Accuracy: 0.4833, Validation Accuracy: 0.4133, Loss: 1.6228\n",
      "Epoch  65 Batch    2/4 - Train Accuracy: 0.5000, Validation Accuracy: 0.4133, Loss: 1.5963\n",
      "Epoch  66 Batch    2/4 - Train Accuracy: 0.4333, Validation Accuracy: 0.4133, Loss: 1.5593\n",
      "Epoch  67 Batch    2/4 - Train Accuracy: 0.5500, Validation Accuracy: 0.4800, Loss: 1.5995\n",
      "Epoch  68 Batch    2/4 - Train Accuracy: 0.4583, Validation Accuracy: 0.4400, Loss: 1.6373\n",
      "Epoch  69 Batch    2/4 - Train Accuracy: 0.4333, Validation Accuracy: 0.4267, Loss: 1.6146\n",
      "Epoch  70 Batch    2/4 - Train Accuracy: 0.4917, Validation Accuracy: 0.4067, Loss: 1.5845\n",
      "Epoch  71 Batch    2/4 - Train Accuracy: 0.4500, Validation Accuracy: 0.4333, Loss: 1.5125\n",
      "Epoch  72 Batch    2/4 - Train Accuracy: 0.4667, Validation Accuracy: 0.4133, Loss: 1.6368\n",
      "Epoch  73 Batch    2/4 - Train Accuracy: 0.4917, Validation Accuracy: 0.4200, Loss: 1.5785\n",
      "Epoch  74 Batch    2/4 - Train Accuracy: 0.4417, Validation Accuracy: 0.4200, Loss: 1.5018\n",
      "Epoch  75 Batch    2/4 - Train Accuracy: 0.5000, Validation Accuracy: 0.4067, Loss: 1.5892\n",
      "Epoch  76 Batch    2/4 - Train Accuracy: 0.4750, Validation Accuracy: 0.4133, Loss: 1.5349\n",
      "Epoch  77 Batch    2/4 - Train Accuracy: 0.4500, Validation Accuracy: 0.4200, Loss: 1.5584\n",
      "Epoch  78 Batch    2/4 - Train Accuracy: 0.5417, Validation Accuracy: 0.4467, Loss: 1.6139\n",
      "Epoch  79 Batch    2/4 - Train Accuracy: 0.5000, Validation Accuracy: 0.4067, Loss: 1.4851\n",
      "Epoch  80 Batch    2/4 - Train Accuracy: 0.5250, Validation Accuracy: 0.4200, Loss: 1.5635\n",
      "Epoch  81 Batch    2/4 - Train Accuracy: 0.5167, Validation Accuracy: 0.4133, Loss: 1.5059\n",
      "Epoch  82 Batch    2/4 - Train Accuracy: 0.4833, Validation Accuracy: 0.4200, Loss: 1.4646\n",
      "Epoch  83 Batch    2/4 - Train Accuracy: 0.5250, Validation Accuracy: 0.4133, Loss: 1.4702\n",
      "Epoch  84 Batch    2/4 - Train Accuracy: 0.5083, Validation Accuracy: 0.4200, Loss: 1.4751\n",
      "Epoch  85 Batch    2/4 - Train Accuracy: 0.5250, Validation Accuracy: 0.4133, Loss: 1.4212\n",
      "Epoch  86 Batch    2/4 - Train Accuracy: 0.5333, Validation Accuracy: 0.4133, Loss: 1.4641\n",
      "Epoch  87 Batch    2/4 - Train Accuracy: 0.5917, Validation Accuracy: 0.4733, Loss: 1.5022\n",
      "Epoch  88 Batch    2/4 - Train Accuracy: 0.5333, Validation Accuracy: 0.4133, Loss: 1.4842\n",
      "Epoch  89 Batch    2/4 - Train Accuracy: 0.5583, Validation Accuracy: 0.4600, Loss: 1.4665\n",
      "Epoch  90 Batch    2/4 - Train Accuracy: 0.5083, Validation Accuracy: 0.4200, Loss: 1.6173\n",
      "Epoch  91 Batch    2/4 - Train Accuracy: 0.5333, Validation Accuracy: 0.4133, Loss: 1.3956\n",
      "Epoch  92 Batch    2/4 - Train Accuracy: 0.5750, Validation Accuracy: 0.4600, Loss: 1.4738\n",
      "Epoch  93 Batch    2/4 - Train Accuracy: 0.5417, Validation Accuracy: 0.4333, Loss: 1.4336\n",
      "Epoch  94 Batch    2/4 - Train Accuracy: 0.6083, Validation Accuracy: 0.4600, Loss: 1.4486\n",
      "Epoch  95 Batch    2/4 - Train Accuracy: 0.6167, Validation Accuracy: 0.4733, Loss: 1.4639\n",
      "Epoch  96 Batch    2/4 - Train Accuracy: 0.4833, Validation Accuracy: 0.4067, Loss: 1.3575\n",
      "Epoch  97 Batch    2/4 - Train Accuracy: 0.6083, Validation Accuracy: 0.4667, Loss: 1.4700\n",
      "Epoch  98 Batch    2/4 - Train Accuracy: 0.4833, Validation Accuracy: 0.4067, Loss: 1.3609\n",
      "Epoch  99 Batch    2/4 - Train Accuracy: 0.5583, Validation Accuracy: 0.4267, Loss: 1.4658\n",
      "Epoch 100 Batch    2/4 - Train Accuracy: 0.5583, Validation Accuracy: 0.4333, Loss: 1.4248\n",
      "Epoch 101 Batch    2/4 - Train Accuracy: 0.5417, Validation Accuracy: 0.4067, Loss: 1.4142\n",
      "Epoch 102 Batch    2/4 - Train Accuracy: 0.6083, Validation Accuracy: 0.4067, Loss: 1.3886\n",
      "Epoch 103 Batch    2/4 - Train Accuracy: 0.5833, Validation Accuracy: 0.4133, Loss: 1.3520\n",
      "Epoch 104 Batch    2/4 - Train Accuracy: 0.5500, Validation Accuracy: 0.4067, Loss: 1.3594\n",
      "Epoch 105 Batch    2/4 - Train Accuracy: 0.6083, Validation Accuracy: 0.4667, Loss: 1.3740\n",
      "Epoch 106 Batch    2/4 - Train Accuracy: 0.5583, Validation Accuracy: 0.4067, Loss: 1.3022\n",
      "Epoch 107 Batch    2/4 - Train Accuracy: 0.5750, Validation Accuracy: 0.4467, Loss: 1.3450\n",
      "Epoch 108 Batch    2/4 - Train Accuracy: 0.5917, Validation Accuracy: 0.4667, Loss: 1.3706\n",
      "Epoch 109 Batch    2/4 - Train Accuracy: 0.5917, Validation Accuracy: 0.4267, Loss: 1.4923\n",
      "Epoch 110 Batch    2/4 - Train Accuracy: 0.6167, Validation Accuracy: 0.4467, Loss: 1.4050\n",
      "Epoch 111 Batch    2/4 - Train Accuracy: 0.6167, Validation Accuracy: 0.4267, Loss: 1.3906\n",
      "Epoch 112 Batch    2/4 - Train Accuracy: 0.6250, Validation Accuracy: 0.4600, Loss: 1.4085\n",
      "Epoch 113 Batch    2/4 - Train Accuracy: 0.6333, Validation Accuracy: 0.4600, Loss: 1.2995\n",
      "Epoch 114 Batch    2/4 - Train Accuracy: 0.6000, Validation Accuracy: 0.4067, Loss: 1.3236\n",
      "Epoch 115 Batch    2/4 - Train Accuracy: 0.6083, Validation Accuracy: 0.4333, Loss: 1.4248\n",
      "Epoch 116 Batch    2/4 - Train Accuracy: 0.5833, Validation Accuracy: 0.4533, Loss: 1.3704\n",
      "Epoch 117 Batch    2/4 - Train Accuracy: 0.6083, Validation Accuracy: 0.4600, Loss: 1.3135\n",
      "Epoch 118 Batch    2/4 - Train Accuracy: 0.5833, Validation Accuracy: 0.4133, Loss: 1.4446\n",
      "Epoch 119 Batch    2/4 - Train Accuracy: 0.6333, Validation Accuracy: 0.4867, Loss: 1.2930\n",
      "Epoch 120 Batch    2/4 - Train Accuracy: 0.5750, Validation Accuracy: 0.4067, Loss: 1.3300\n",
      "Epoch 121 Batch    2/4 - Train Accuracy: 0.6417, Validation Accuracy: 0.4800, Loss: 1.3618\n",
      "Epoch 122 Batch    2/4 - Train Accuracy: 0.6333, Validation Accuracy: 0.4800, Loss: 1.3240\n",
      "Epoch 123 Batch    2/4 - Train Accuracy: 0.5667, Validation Accuracy: 0.4267, Loss: 1.2883\n",
      "Epoch 124 Batch    2/4 - Train Accuracy: 0.6417, Validation Accuracy: 0.4800, Loss: 1.2284\n",
      "Epoch 125 Batch    2/4 - Train Accuracy: 0.6250, Validation Accuracy: 0.4600, Loss: 1.3195\n",
      "Epoch 126 Batch    2/4 - Train Accuracy: 0.5833, Validation Accuracy: 0.4333, Loss: 1.2804\n",
      "Epoch 127 Batch    2/4 - Train Accuracy: 0.6083, Validation Accuracy: 0.4867, Loss: 1.2838\n",
      "Epoch 128 Batch    2/4 - Train Accuracy: 0.5917, Validation Accuracy: 0.4400, Loss: 1.2691\n",
      "Epoch 129 Batch    2/4 - Train Accuracy: 0.6250, Validation Accuracy: 0.4600, Loss: 1.2046\n",
      "Epoch 130 Batch    2/4 - Train Accuracy: 0.5917, Validation Accuracy: 0.4267, Loss: 1.2731\n",
      "Epoch 131 Batch    2/4 - Train Accuracy: 0.6083, Validation Accuracy: 0.4200, Loss: 1.2930\n",
      "Epoch 132 Batch    2/4 - Train Accuracy: 0.6417, Validation Accuracy: 0.4867, Loss: 1.2491\n",
      "Epoch 133 Batch    2/4 - Train Accuracy: 0.6583, Validation Accuracy: 0.4800, Loss: 1.2323\n",
      "Epoch 134 Batch    2/4 - Train Accuracy: 0.6583, Validation Accuracy: 0.4733, Loss: 1.2105\n",
      "Epoch 135 Batch    2/4 - Train Accuracy: 0.6750, Validation Accuracy: 0.4733, Loss: 1.2624\n",
      "Epoch 136 Batch    2/4 - Train Accuracy: 0.6417, Validation Accuracy: 0.4600, Loss: 1.2669\n",
      "Epoch 137 Batch    2/4 - Train Accuracy: 0.6417, Validation Accuracy: 0.4400, Loss: 1.2070\n",
      "Epoch 138 Batch    2/4 - Train Accuracy: 0.6250, Validation Accuracy: 0.4800, Loss: 1.2036\n",
      "Epoch 139 Batch    2/4 - Train Accuracy: 0.6167, Validation Accuracy: 0.4667, Loss: 1.2699\n",
      "Epoch 140 Batch    2/4 - Train Accuracy: 0.6500, Validation Accuracy: 0.4733, Loss: 1.1763\n",
      "Epoch 141 Batch    2/4 - Train Accuracy: 0.6250, Validation Accuracy: 0.4267, Loss: 1.1305\n",
      "Epoch 142 Batch    2/4 - Train Accuracy: 0.6333, Validation Accuracy: 0.5000, Loss: 1.1910\n",
      "Epoch 143 Batch    2/4 - Train Accuracy: 0.6500, Validation Accuracy: 0.4600, Loss: 1.2472\n",
      "Epoch 144 Batch    2/4 - Train Accuracy: 0.6083, Validation Accuracy: 0.4267, Loss: 1.2007\n",
      "Epoch 145 Batch    2/4 - Train Accuracy: 0.6417, Validation Accuracy: 0.4800, Loss: 1.1307\n",
      "Epoch 146 Batch    2/4 - Train Accuracy: 0.6500, Validation Accuracy: 0.4533, Loss: 1.1258\n",
      "Epoch 147 Batch    2/4 - Train Accuracy: 0.6250, Validation Accuracy: 0.4267, Loss: 1.2053\n",
      "Epoch 148 Batch    2/4 - Train Accuracy: 0.6667, Validation Accuracy: 0.4933, Loss: 1.1171\n",
      "Epoch 149 Batch    2/4 - Train Accuracy: 0.6250, Validation Accuracy: 0.4333, Loss: 1.1203\n",
      "Epoch 150 Batch    2/4 - Train Accuracy: 0.6500, Validation Accuracy: 0.4600, Loss: 1.1877\n",
      "Epoch 151 Batch    2/4 - Train Accuracy: 0.6667, Validation Accuracy: 0.4933, Loss: 1.1529\n",
      "Epoch 152 Batch    2/4 - Train Accuracy: 0.6750, Validation Accuracy: 0.4400, Loss: 1.1489\n",
      "Epoch 153 Batch    2/4 - Train Accuracy: 0.7083, Validation Accuracy: 0.4933, Loss: 1.0665\n",
      "Epoch 154 Batch    2/4 - Train Accuracy: 0.6833, Validation Accuracy: 0.5000, Loss: 1.2301\n",
      "Epoch 155 Batch    2/4 - Train Accuracy: 0.6333, Validation Accuracy: 0.4467, Loss: 1.1173\n",
      "Epoch 156 Batch    2/4 - Train Accuracy: 0.6833, Validation Accuracy: 0.5000, Loss: 1.1223\n",
      "Epoch 157 Batch    2/4 - Train Accuracy: 0.6667, Validation Accuracy: 0.4400, Loss: 1.0215\n",
      "Epoch 158 Batch    2/4 - Train Accuracy: 0.6833, Validation Accuracy: 0.4933, Loss: 1.1530\n",
      "Epoch 159 Batch    2/4 - Train Accuracy: 0.6667, Validation Accuracy: 0.4667, Loss: 1.1160\n",
      "Epoch 160 Batch    2/4 - Train Accuracy: 0.7000, Validation Accuracy: 0.4867, Loss: 1.0952\n",
      "Epoch 161 Batch    2/4 - Train Accuracy: 0.7000, Validation Accuracy: 0.4867, Loss: 1.1099\n",
      "Epoch 162 Batch    2/4 - Train Accuracy: 0.7000, Validation Accuracy: 0.4867, Loss: 1.0472\n",
      "Epoch 163 Batch    2/4 - Train Accuracy: 0.6833, Validation Accuracy: 0.4800, Loss: 1.0747\n",
      "Epoch 164 Batch    2/4 - Train Accuracy: 0.7000, Validation Accuracy: 0.4667, Loss: 1.0851\n",
      "Epoch 165 Batch    2/4 - Train Accuracy: 0.6750, Validation Accuracy: 0.4333, Loss: 1.0427\n",
      "Epoch 166 Batch    2/4 - Train Accuracy: 0.7000, Validation Accuracy: 0.5000, Loss: 1.0926\n",
      "Epoch 167 Batch    2/4 - Train Accuracy: 0.7083, Validation Accuracy: 0.4867, Loss: 1.0643\n",
      "Epoch 168 Batch    2/4 - Train Accuracy: 0.6917, Validation Accuracy: 0.4533, Loss: 1.0392\n",
      "Epoch 169 Batch    2/4 - Train Accuracy: 0.7000, Validation Accuracy: 0.5000, Loss: 1.0615\n",
      "Epoch 170 Batch    2/4 - Train Accuracy: 0.7000, Validation Accuracy: 0.4600, Loss: 1.0266\n",
      "Epoch 171 Batch    2/4 - Train Accuracy: 0.6833, Validation Accuracy: 0.4667, Loss: 1.0287\n",
      "Epoch 172 Batch    2/4 - Train Accuracy: 0.6750, Validation Accuracy: 0.4867, Loss: 1.0281\n",
      "Epoch 173 Batch    2/4 - Train Accuracy: 0.6917, Validation Accuracy: 0.4733, Loss: 1.0802\n",
      "Epoch 174 Batch    2/4 - Train Accuracy: 0.7083, Validation Accuracy: 0.4733, Loss: 1.1418\n",
      "Epoch 175 Batch    2/4 - Train Accuracy: 0.7083, Validation Accuracy: 0.4667, Loss: 1.0208\n",
      "Epoch 176 Batch    2/4 - Train Accuracy: 0.7333, Validation Accuracy: 0.5000, Loss: 0.9914\n",
      "Epoch 177 Batch    2/4 - Train Accuracy: 0.7333, Validation Accuracy: 0.5067, Loss: 1.0144\n",
      "Epoch 178 Batch    2/4 - Train Accuracy: 0.7333, Validation Accuracy: 0.4733, Loss: 1.0061\n",
      "Epoch 179 Batch    2/4 - Train Accuracy: 0.7083, Validation Accuracy: 0.4733, Loss: 1.1097\n",
      "Epoch 180 Batch    2/4 - Train Accuracy: 0.7167, Validation Accuracy: 0.5133, Loss: 1.0364\n",
      "Epoch 181 Batch    2/4 - Train Accuracy: 0.7000, Validation Accuracy: 0.4533, Loss: 1.0022\n",
      "Epoch 182 Batch    2/4 - Train Accuracy: 0.7167, Validation Accuracy: 0.4867, Loss: 0.9783\n",
      "Epoch 183 Batch    2/4 - Train Accuracy: 0.7083, Validation Accuracy: 0.4733, Loss: 0.9816\n",
      "Epoch 184 Batch    2/4 - Train Accuracy: 0.7083, Validation Accuracy: 0.4733, Loss: 1.0685\n",
      "Epoch 185 Batch    2/4 - Train Accuracy: 0.7167, Validation Accuracy: 0.5133, Loss: 1.0101\n",
      "Epoch 186 Batch    2/4 - Train Accuracy: 0.7000, Validation Accuracy: 0.4600, Loss: 0.9762\n",
      "Epoch 187 Batch    2/4 - Train Accuracy: 0.6750, Validation Accuracy: 0.5067, Loss: 0.9765\n",
      "Epoch 188 Batch    2/4 - Train Accuracy: 0.7083, Validation Accuracy: 0.4733, Loss: 1.0383\n",
      "Epoch 189 Batch    2/4 - Train Accuracy: 0.7167, Validation Accuracy: 0.4400, Loss: 1.0016\n",
      "Epoch 190 Batch    2/4 - Train Accuracy: 0.7250, Validation Accuracy: 0.4867, Loss: 1.0214\n",
      "Epoch 191 Batch    2/4 - Train Accuracy: 0.7083, Validation Accuracy: 0.4800, Loss: 0.9339\n",
      "Epoch 192 Batch    2/4 - Train Accuracy: 0.7083, Validation Accuracy: 0.4667, Loss: 1.0888\n",
      "Epoch 193 Batch    2/4 - Train Accuracy: 0.7083, Validation Accuracy: 0.5133, Loss: 0.9759\n",
      "Epoch 194 Batch    2/4 - Train Accuracy: 0.7083, Validation Accuracy: 0.4667, Loss: 0.9696\n",
      "Epoch 195 Batch    2/4 - Train Accuracy: 0.7167, Validation Accuracy: 0.4667, Loss: 0.9556\n",
      "Epoch 196 Batch    2/4 - Train Accuracy: 0.7000, Validation Accuracy: 0.5000, Loss: 1.0369\n",
      "Epoch 197 Batch    2/4 - Train Accuracy: 0.7000, Validation Accuracy: 0.4933, Loss: 0.9692\n",
      "Epoch 198 Batch    2/4 - Train Accuracy: 0.7250, Validation Accuracy: 0.4600, Loss: 0.9492\n",
      "Epoch 199 Batch    2/4 - Train Accuracy: 0.7167, Validation Accuracy: 0.4867, Loss: 0.8548\n",
      "Epoch 200 Batch    2/4 - Train Accuracy: 0.7167, Validation Accuracy: 0.4667, Loss: 0.9462\n",
      "Epoch 201 Batch    2/4 - Train Accuracy: 0.7083, Validation Accuracy: 0.5067, Loss: 1.0033\n",
      "Epoch 202 Batch    2/4 - Train Accuracy: 0.7083, Validation Accuracy: 0.5000, Loss: 0.8883\n",
      "Epoch 203 Batch    2/4 - Train Accuracy: 0.7250, Validation Accuracy: 0.4533, Loss: 0.9868\n",
      "Epoch 204 Batch    2/4 - Train Accuracy: 0.7083, Validation Accuracy: 0.4933, Loss: 0.9133\n",
      "Epoch 205 Batch    2/4 - Train Accuracy: 0.7333, Validation Accuracy: 0.4733, Loss: 0.8706\n",
      "Epoch 206 Batch    2/4 - Train Accuracy: 0.7250, Validation Accuracy: 0.4533, Loss: 0.9045\n",
      "Epoch 207 Batch    2/4 - Train Accuracy: 0.7167, Validation Accuracy: 0.5133, Loss: 0.8791\n",
      "Epoch 208 Batch    2/4 - Train Accuracy: 0.7333, Validation Accuracy: 0.4667, Loss: 0.9424\n",
      "Epoch 209 Batch    2/4 - Train Accuracy: 0.7250, Validation Accuracy: 0.4933, Loss: 0.8695\n",
      "Epoch 210 Batch    2/4 - Train Accuracy: 0.7083, Validation Accuracy: 0.5200, Loss: 0.9501\n",
      "Epoch 211 Batch    2/4 - Train Accuracy: 0.6583, Validation Accuracy: 0.4333, Loss: 0.9152\n",
      "Epoch 212 Batch    2/4 - Train Accuracy: 0.7417, Validation Accuracy: 0.5133, Loss: 1.0241\n",
      "Epoch 213 Batch    2/4 - Train Accuracy: 0.7333, Validation Accuracy: 0.5267, Loss: 0.9823\n",
      "Epoch 214 Batch    2/4 - Train Accuracy: 0.7417, Validation Accuracy: 0.4667, Loss: 0.8612\n",
      "Epoch 215 Batch    2/4 - Train Accuracy: 0.7333, Validation Accuracy: 0.5000, Loss: 0.9245\n",
      "Epoch 216 Batch    2/4 - Train Accuracy: 0.7333, Validation Accuracy: 0.5133, Loss: 0.9514\n",
      "Epoch 217 Batch    2/4 - Train Accuracy: 0.7333, Validation Accuracy: 0.4733, Loss: 0.9190\n",
      "Epoch 218 Batch    2/4 - Train Accuracy: 0.7417, Validation Accuracy: 0.4867, Loss: 0.8716\n",
      "Epoch 219 Batch    2/4 - Train Accuracy: 0.7167, Validation Accuracy: 0.4600, Loss: 0.8440\n",
      "Epoch 220 Batch    2/4 - Train Accuracy: 0.7500, Validation Accuracy: 0.5200, Loss: 0.8791\n",
      "Epoch 221 Batch    2/4 - Train Accuracy: 0.7500, Validation Accuracy: 0.5000, Loss: 0.8735\n",
      "Epoch 222 Batch    2/4 - Train Accuracy: 0.7333, Validation Accuracy: 0.4933, Loss: 0.8683\n",
      "Epoch 223 Batch    2/4 - Train Accuracy: 0.7333, Validation Accuracy: 0.4867, Loss: 0.8981\n",
      "Epoch 224 Batch    2/4 - Train Accuracy: 0.7333, Validation Accuracy: 0.4933, Loss: 0.9126\n",
      "Epoch 225 Batch    2/4 - Train Accuracy: 0.7500, Validation Accuracy: 0.5067, Loss: 0.8618\n",
      "Epoch 226 Batch    2/4 - Train Accuracy: 0.7667, Validation Accuracy: 0.4800, Loss: 0.9750\n",
      "Epoch 227 Batch    2/4 - Train Accuracy: 0.7500, Validation Accuracy: 0.5133, Loss: 0.8818\n",
      "Epoch 228 Batch    2/4 - Train Accuracy: 0.7250, Validation Accuracy: 0.4867, Loss: 0.8159\n",
      "Epoch 229 Batch    2/4 - Train Accuracy: 0.7333, Validation Accuracy: 0.4867, Loss: 0.8938\n",
      "Epoch 230 Batch    2/4 - Train Accuracy: 0.7500, Validation Accuracy: 0.5267, Loss: 0.8520\n",
      "Epoch 231 Batch    2/4 - Train Accuracy: 0.7417, Validation Accuracy: 0.5067, Loss: 0.8758\n",
      "Epoch 232 Batch    2/4 - Train Accuracy: 0.7083, Validation Accuracy: 0.5133, Loss: 0.8735\n",
      "Epoch 233 Batch    2/4 - Train Accuracy: 0.7083, Validation Accuracy: 0.4867, Loss: 0.8010\n",
      "Epoch 234 Batch    2/4 - Train Accuracy: 0.7167, Validation Accuracy: 0.5000, Loss: 0.7867\n",
      "Epoch 235 Batch    2/4 - Train Accuracy: 0.7500, Validation Accuracy: 0.5200, Loss: 0.8303\n",
      "Epoch 236 Batch    2/4 - Train Accuracy: 0.7833, Validation Accuracy: 0.5067, Loss: 0.7803\n",
      "Epoch 237 Batch    2/4 - Train Accuracy: 0.7500, Validation Accuracy: 0.5067, Loss: 0.8143\n",
      "Epoch 238 Batch    2/4 - Train Accuracy: 0.7417, Validation Accuracy: 0.5133, Loss: 0.8047\n",
      "Epoch 239 Batch    2/4 - Train Accuracy: 0.7167, Validation Accuracy: 0.5200, Loss: 0.8162\n",
      "Epoch 240 Batch    2/4 - Train Accuracy: 0.7000, Validation Accuracy: 0.4933, Loss: 0.7804\n",
      "Epoch 241 Batch    2/4 - Train Accuracy: 0.6917, Validation Accuracy: 0.5200, Loss: 0.7979\n",
      "Epoch 242 Batch    2/4 - Train Accuracy: 0.7583, Validation Accuracy: 0.4867, Loss: 0.8610\n",
      "Epoch 243 Batch    2/4 - Train Accuracy: 0.7667, Validation Accuracy: 0.5133, Loss: 0.7733\n",
      "Epoch 244 Batch    2/4 - Train Accuracy: 0.7667, Validation Accuracy: 0.5133, Loss: 0.7882\n",
      "Epoch 245 Batch    2/4 - Train Accuracy: 0.7667, Validation Accuracy: 0.4800, Loss: 0.7833\n",
      "Epoch 246 Batch    2/4 - Train Accuracy: 0.7833, Validation Accuracy: 0.5133, Loss: 0.7996\n",
      "Epoch 247 Batch    2/4 - Train Accuracy: 0.7583, Validation Accuracy: 0.5067, Loss: 0.8333\n",
      "Epoch 248 Batch    2/4 - Train Accuracy: 0.7667, Validation Accuracy: 0.5000, Loss: 0.7945\n",
      "Epoch 249 Batch    2/4 - Train Accuracy: 0.7583, Validation Accuracy: 0.5133, Loss: 0.7787\n",
      "Epoch 250 Batch    2/4 - Train Accuracy: 0.7583, Validation Accuracy: 0.5133, Loss: 0.7636\n",
      "Epoch 251 Batch    2/4 - Train Accuracy: 0.7667, Validation Accuracy: 0.4867, Loss: 0.7742\n",
      "Epoch 252 Batch    2/4 - Train Accuracy: 0.7833, Validation Accuracy: 0.5267, Loss: 0.7765\n",
      "Epoch 253 Batch    2/4 - Train Accuracy: 0.7917, Validation Accuracy: 0.4933, Loss: 0.8399\n",
      "Epoch 254 Batch    2/4 - Train Accuracy: 0.7750, Validation Accuracy: 0.4933, Loss: 0.7338\n",
      "Epoch 255 Batch    2/4 - Train Accuracy: 0.7833, Validation Accuracy: 0.5267, Loss: 0.8141\n",
      "Epoch 256 Batch    2/4 - Train Accuracy: 0.7750, Validation Accuracy: 0.5133, Loss: 0.8410\n",
      "Epoch 257 Batch    2/4 - Train Accuracy: 0.7417, Validation Accuracy: 0.5067, Loss: 0.7515\n",
      "Epoch 258 Batch    2/4 - Train Accuracy: 0.7667, Validation Accuracy: 0.5200, Loss: 0.7602\n",
      "Epoch 259 Batch    2/4 - Train Accuracy: 0.7500, Validation Accuracy: 0.4800, Loss: 0.7332\n",
      "Epoch 260 Batch    2/4 - Train Accuracy: 0.7833, Validation Accuracy: 0.5267, Loss: 0.8832\n",
      "Epoch 261 Batch    2/4 - Train Accuracy: 0.7667, Validation Accuracy: 0.5267, Loss: 0.7525\n",
      "Epoch 262 Batch    2/4 - Train Accuracy: 0.7917, Validation Accuracy: 0.5200, Loss: 0.8205\n",
      "Epoch 263 Batch    2/4 - Train Accuracy: 0.7500, Validation Accuracy: 0.5333, Loss: 0.7405\n",
      "Epoch 264 Batch    2/4 - Train Accuracy: 0.7583, Validation Accuracy: 0.5267, Loss: 0.9107\n",
      "Epoch 265 Batch    2/4 - Train Accuracy: 0.7750, Validation Accuracy: 0.5200, Loss: 0.7185\n",
      "Epoch 266 Batch    2/4 - Train Accuracy: 0.7667, Validation Accuracy: 0.5400, Loss: 0.7382\n",
      "Epoch 267 Batch    2/4 - Train Accuracy: 0.7833, Validation Accuracy: 0.5333, Loss: 0.6784\n",
      "Epoch 268 Batch    2/4 - Train Accuracy: 0.7667, Validation Accuracy: 0.4800, Loss: 0.8838\n",
      "Epoch 269 Batch    2/4 - Train Accuracy: 0.7750, Validation Accuracy: 0.5333, Loss: 0.6960\n",
      "Epoch 270 Batch    2/4 - Train Accuracy: 0.7750, Validation Accuracy: 0.5000, Loss: 0.7719\n",
      "Epoch 271 Batch    2/4 - Train Accuracy: 0.7667, Validation Accuracy: 0.4933, Loss: 0.7470\n",
      "Epoch 272 Batch    2/4 - Train Accuracy: 0.7917, Validation Accuracy: 0.5200, Loss: 0.8496\n",
      "Epoch 273 Batch    2/4 - Train Accuracy: 0.7750, Validation Accuracy: 0.5200, Loss: 0.7643\n",
      "Epoch 274 Batch    2/4 - Train Accuracy: 0.7667, Validation Accuracy: 0.4933, Loss: 0.6733\n",
      "Epoch 275 Batch    2/4 - Train Accuracy: 0.7667, Validation Accuracy: 0.5200, Loss: 0.6948\n",
      "Epoch 276 Batch    2/4 - Train Accuracy: 0.8083, Validation Accuracy: 0.5400, Loss: 0.8013\n",
      "Epoch 277 Batch    2/4 - Train Accuracy: 0.7417, Validation Accuracy: 0.5133, Loss: 0.7370\n",
      "Epoch 278 Batch    2/4 - Train Accuracy: 0.7750, Validation Accuracy: 0.5200, Loss: 0.8196\n",
      "Epoch 279 Batch    2/4 - Train Accuracy: 0.8083, Validation Accuracy: 0.5200, Loss: 0.7483\n",
      "Epoch 280 Batch    2/4 - Train Accuracy: 0.7750, Validation Accuracy: 0.5333, Loss: 0.6159\n",
      "Epoch 281 Batch    2/4 - Train Accuracy: 0.7833, Validation Accuracy: 0.5333, Loss: 0.7778\n",
      "Epoch 282 Batch    2/4 - Train Accuracy: 0.7917, Validation Accuracy: 0.5000, Loss: 0.6433\n",
      "Epoch 283 Batch    2/4 - Train Accuracy: 0.8083, Validation Accuracy: 0.5067, Loss: 0.7462\n",
      "Epoch 284 Batch    2/4 - Train Accuracy: 0.7583, Validation Accuracy: 0.5200, Loss: 0.7077\n",
      "Epoch 285 Batch    2/4 - Train Accuracy: 0.8083, Validation Accuracy: 0.5000, Loss: 0.7336\n",
      "Epoch 286 Batch    2/4 - Train Accuracy: 0.8083, Validation Accuracy: 0.5067, Loss: 0.7761\n",
      "Epoch 287 Batch    2/4 - Train Accuracy: 0.8000, Validation Accuracy: 0.5333, Loss: 0.6943\n",
      "Epoch 288 Batch    2/4 - Train Accuracy: 0.7833, Validation Accuracy: 0.5067, Loss: 0.6393\n",
      "Epoch 289 Batch    2/4 - Train Accuracy: 0.7833, Validation Accuracy: 0.5200, Loss: 0.6483\n",
      "Epoch 290 Batch    2/4 - Train Accuracy: 0.8083, Validation Accuracy: 0.5133, Loss: 0.7052\n",
      "Epoch 291 Batch    2/4 - Train Accuracy: 0.8000, Validation Accuracy: 0.5200, Loss: 0.6244\n",
      "Epoch 292 Batch    2/4 - Train Accuracy: 0.7417, Validation Accuracy: 0.5400, Loss: 0.6678\n",
      "Epoch 293 Batch    2/4 - Train Accuracy: 0.7833, Validation Accuracy: 0.5400, Loss: 0.7511\n",
      "Epoch 294 Batch    2/4 - Train Accuracy: 0.8000, Validation Accuracy: 0.5533, Loss: 0.6674\n",
      "Epoch 295 Batch    2/4 - Train Accuracy: 0.8167, Validation Accuracy: 0.4867, Loss: 0.5736\n",
      "Epoch 296 Batch    2/4 - Train Accuracy: 0.8417, Validation Accuracy: 0.4933, Loss: 0.6359\n",
      "Epoch 297 Batch    2/4 - Train Accuracy: 0.8167, Validation Accuracy: 0.5133, Loss: 0.8344\n",
      "Epoch 298 Batch    2/4 - Train Accuracy: 0.8333, Validation Accuracy: 0.4667, Loss: 0.7157\n",
      "Epoch 299 Batch    2/4 - Train Accuracy: 0.8333, Validation Accuracy: 0.4800, Loss: 0.7146\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-dcec19262a7d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;31m# Save Model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[0msaver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m     \u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Model Trained and Saved'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state, strip_default_attrs)\u001b[0m\n\u001b[0;32m   1194\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1195\u001b[0m           self.export_meta_graph(\n\u001b[1;32m-> 1196\u001b[1;33m               meta_graph_filename, strip_default_attrs=strip_default_attrs)\n\u001b[0m\u001b[0;32m   1197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1198\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_empty\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36mexport_meta_graph\u001b[1;34m(self, filename, collection_list, as_text, export_scope, clear_devices, clear_extraneous_savers, strip_default_attrs)\u001b[0m\n\u001b[0;32m   1232\u001b[0m     return export_meta_graph(\n\u001b[0;32m   1233\u001b[0m         \u001b[0mfilename\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1234\u001b[1;33m         \u001b[0mgraph_def\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_graph_def\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madd_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1235\u001b[0m         \u001b[0msaver_def\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1236\u001b[0m         \u001b[0mcollection_list\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollection_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mas_graph_def\u001b[1;34m(self, from_version, add_shapes)\u001b[0m\n\u001b[0;32m   3148\u001b[0m     \"\"\"\n\u001b[0;32m   3149\u001b[0m     \u001b[1;31m# pylint: enable=line-too-long\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3150\u001b[1;33m     \u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_as_graph_def\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfrom_version\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_shapes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3151\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3152\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_as_graph_def\u001b[1;34m(self, from_version, add_shapes)\u001b[0m\n\u001b[0;32m   3109\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3110\u001b[0m       \u001b[0mgraph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_pb2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGraphDef\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3111\u001b[1;33m       \u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mParseFromString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3112\u001b[0m       \u001b[1;31m# Strip the experimental library field iff it's empty.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3113\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlibrary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\google\\protobuf\\message.py\u001b[0m in \u001b[0;36mParseFromString\u001b[1;34m(self, serialized)\u001b[0m\n\u001b[0;32m    185\u001b[0m     \"\"\"\n\u001b[0;32m    186\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mClear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMergeFromString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mserialized\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mSerializeToString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\google\\protobuf\\internal\\python_message.py\u001b[0m in \u001b[0;36mMergeFromString\u001b[1;34m(self, serialized)\u001b[0m\n\u001b[0;32m   1143\u001b[0m     \u001b[0mlength\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mserialized\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1144\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1145\u001b[1;33m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_InternalParse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mserialized\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlength\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1146\u001b[0m         \u001b[1;31m# The only reason _InternalParse would return early is if it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1147\u001b[0m         \u001b[1;31m# encountered an end-group tag.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\google\\protobuf\\internal\\python_message.py\u001b[0m in \u001b[0;36mInternalParse\u001b[1;34m(self, buffer, pos, end)\u001b[0m\n\u001b[0;32m   1208\u001b[0m         \u001b[0mpos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_pos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1209\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1210\u001b[1;33m         \u001b[0mpos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfield_decoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_pos\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfield_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1211\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfield_desc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1212\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_UpdateOneofState\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfield_desc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\google\\protobuf\\internal\\decoder.py\u001b[0m in \u001b[0;36mDecodeRepeatedField\u001b[1;34m(buffer, pos, end, message, field_dict)\u001b[0m\n\u001b[0;32m    700\u001b[0m           \u001b[1;32mraise\u001b[0m \u001b[0m_DecodeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Truncated message.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m         \u001b[1;31m# Read sub-message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_InternalParse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_pos\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mnew_pos\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    703\u001b[0m           \u001b[1;31m# The only reason _InternalParse would return early is if it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m           \u001b[1;31m# encountered an end-group tag.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\google\\protobuf\\internal\\python_message.py\u001b[0m in \u001b[0;36mInternalParse\u001b[1;34m(self, buffer, pos, end)\u001b[0m\n\u001b[0;32m   1208\u001b[0m         \u001b[0mpos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_pos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1209\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1210\u001b[1;33m         \u001b[0mpos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfield_decoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_pos\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfield_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1211\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfield_desc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1212\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_UpdateOneofState\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfield_desc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\google\\protobuf\\internal\\decoder.py\u001b[0m in \u001b[0;36mDecodeRepeatedField\u001b[1;34m(buffer, pos, end, message, field_dict)\u001b[0m\n\u001b[0;32m    573\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfield_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m       \u001b[1;32mwhile\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 575\u001b[1;33m         \u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlocal_DecodeVarint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    576\u001b[0m         \u001b[0mnew_pos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpos\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnew_pos\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\google\\protobuf\\internal\\decoder.py\u001b[0m in \u001b[0;36mDecodeVarint\u001b[1;34m(buffer, pos)\u001b[0m\n\u001b[0;32m    125\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m       \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindexbytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m|=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m \u001b[1;33m&\u001b[0m \u001b[1;36m0x7f\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<<\u001b[0m \u001b[0mshift\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    128\u001b[0m       \u001b[0mpos\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m \u001b[1;33m&\u001b[0m \u001b[1;36m0x80\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def get_accuracy(target, logits):\n",
    "    \"\"\"\n",
    "    Calculate accuracy\n",
    "    \"\"\"\n",
    "    max_seq = max(target.shape[1], logits.shape[1])\n",
    "    if max_seq - target.shape[1]:\n",
    "        target = np.pad(\n",
    "            target,\n",
    "            [(0,0),(0,max_seq - target.shape[1])],\n",
    "            'constant')\n",
    "    if max_seq - logits.shape[1]:\n",
    "        logits = np.pad(\n",
    "            logits,\n",
    "            [(0,0),(0,max_seq - logits.shape[1])],\n",
    "            'constant')\n",
    "\n",
    "    return np.mean(np.equal(target, logits))\n",
    "# Split data to training and validation sets\n",
    "train_source = source_int_text[batch_size:]\n",
    "train_target = target_int_text[batch_size:]\n",
    "valid_source = source_int_text[:batch_size]\n",
    "valid_target = target_int_text[:batch_size]\n",
    "(valid_sources_batch, valid_targets_batch, valid_sources_lengths, valid_targets_lengths ) = next(get_batches(valid_source,\n",
    "                                                                                                             valid_target,\n",
    "                                                                                                             batch_size,\n",
    "                                                                                                             source_vocab_to_int['<PAD>'],\n",
    "                                                                                                             target_vocab_to_int['<PAD>']))                                                                                                  \n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch_i in range(epochs):\n",
    "        for batch_i, (source_batch, target_batch, sources_lengths, targets_lengths) in enumerate(\n",
    "                get_batches(train_source, train_target, batch_size,\n",
    "                            source_vocab_to_int['<PAD>'],\n",
    "                            target_vocab_to_int['<PAD>'])):\n",
    "\n",
    "            _, loss = sess.run(\n",
    "                [train_op, cost],\n",
    "                {input_data: source_batch,\n",
    "                 targets: target_batch,\n",
    "                 lr: learning_rate,\n",
    "                 target_sequence_length: targets_lengths,\n",
    "                 keep_prob: keep_probability})\n",
    "\n",
    "\n",
    "            if batch_i % display_step == 0 and batch_i > 0:\n",
    "                batch_train_logits = sess.run(\n",
    "                    inference_logits,\n",
    "                    {input_data: source_batch,\n",
    "                     target_sequence_length: targets_lengths,\n",
    "                     keep_prob: 1.0})\n",
    "\n",
    "                batch_valid_logits = sess.run(\n",
    "                    inference_logits,\n",
    "                    {input_data: valid_sources_batch,\n",
    "                     target_sequence_length: valid_targets_lengths,\n",
    "                     keep_prob: 1.0})\n",
    "\n",
    "                train_acc = get_accuracy(target_batch, batch_train_logits)\n",
    "                valid_acc = get_accuracy(valid_targets_batch, batch_valid_logits)\n",
    "\n",
    "                print('Epoch {:>3} Batch {:>4}/{} - Train Accuracy: {:>6.4f}, Validation Accuracy: {:>6.4f}, Loss: {:>6.4f}'\n",
    "                      .format(epoch_i, batch_i, len(source_int_text) // batch_size, train_acc, valid_acc, loss))\n",
    "\n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, save_path)\n",
    "    print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eg0K0PhQUQ6g"
   },
   "source": [
    "## Code for saving the model for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r_M-CU1cbKLf"
   },
   "outputs": [],
   "source": [
    "def save_params(params):\n",
    "    with open('params.p', 'wb') as out_file:\n",
    "        pickle.dump(params, out_file)\n",
    "\n",
    "\n",
    "def load_params():\n",
    "    with open('params.p', mode='rb') as in_file:\n",
    "        return pickle.load(in_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eol1_ndobKN7"
   },
   "outputs": [],
   "source": [
    "save_params(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W8LKJY6xbXsH"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "#import problem_unittests as tests\n",
    "\n",
    "_, (source_vocab_to_int, target_vocab_to_int), (source_int_to_vocab, target_int_to_vocab) = load_preprocess()\n",
    "load_path = load_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L5sDnlxGUAKh"
   },
   "source": [
    "## Predicting vibrating device input for a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 151
    },
    "colab_type": "code",
    "id": "wJYcZ1bIbXw-",
    "outputId": "926b0c05-9e67-4066-91ae-1a5616e252ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from C:/Users/ROHIT/Desktop/Hackfest/3/model/checkpoints/dev\n",
      "Input\n",
      "  Word Ids:      [8, 35, 23, 28, 24, 12, 35, 21, 34]\n",
      "  English Words: ['bob', 'and', 'mary', 'love', 'eating', 'cheese', 'and', 'chicken', 'burger']\n",
      "\n",
      "Prediction\n",
      "  Word Ids:      [18, 4, 17, 30, 30, 14, 20, 20, 34, 23, 1]\n",
      "  Vibration Binary words: 10001 01111 00110 11100 11100 01001 01011 01011 11111 00101 <EOS>\n"
     ]
    }
   ],
   "source": [
    "def sentence_to_seq(sentence, vocab_to_int):\n",
    "    results = []\n",
    "    for word in sentence.split(\" \"):\n",
    "        if word in vocab_to_int:\n",
    "            results.append(vocab_to_int[word])\n",
    "        else:\n",
    "            results.append(vocab_to_int['<UNK>'])\n",
    "            \n",
    "    return results\n",
    "\n",
    "translate_sentence = 'bob and mary love eating cheese and chicken burger'\n",
    "\n",
    "\n",
    "translate_sentence = sentence_to_seq(translate_sentence, source_vocab_to_int)\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(load_path + '.meta')\n",
    "    loader.restore(sess, load_path)\n",
    "\n",
    "    input_data = loaded_graph.get_tensor_by_name('input:0')\n",
    "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
    "    target_sequence_length = loaded_graph.get_tensor_by_name('target_sequence_length:0')\n",
    "    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "\n",
    "    translate_logits = sess.run(logits, {input_data: [translate_sentence]*batch_size,\n",
    "                                         target_sequence_length: [len(translate_sentence)*2]*batch_size,\n",
    "                                         keep_prob: 1.0})[0]\n",
    "\n",
    "print('Input')\n",
    "print('  Word Ids:      {}'.format([i for i in translate_sentence]))\n",
    "print('  English Words: {}'.format([source_int_to_vocab[i] for i in translate_sentence]))\n",
    "\n",
    "print('\\nPrediction')\n",
    "print('  Word Ids:      {}'.format([i for i in translate_logits]))\n",
    "print('  Vibration Binary words: {}'.format(\" \".join([target_int_to_vocab[i] for i in translate_logits])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3bi-Ep7fo7fZ"
   },
   "outputs": [],
   "source": [
    "def restore(sess_var, model_path):\n",
    "    if model_path is not None:\n",
    "        if os.path.exists(\"{}.index\".format(model_path)):\n",
    "            saver = tf.train.Saver(var_list=tf.trainable_variables())\n",
    "            saver.restore(sess_var, model_path)\n",
    "            print(\"Model at %s restored\" % model_path)\n",
    "        else:\n",
    "            print(\"Model path does not exist, skipping...\")\n",
    "    else:\n",
    "        print(\"Model path is None - Nothing to restore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UU0HBYEQPG2w"
   },
   "source": [
    "# Voice input to text conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4O9kcvyio7kI"
   },
   "outputs": [],
   "source": [
    "import pyaudio\n",
    "import speech_recognition as sr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "atp9NDY9PXGG"
   },
   "source": [
    "## Next code recieves input from microphone stores it in audio file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kl_WFrlWo7md"
   },
   "outputs": [],
   "source": [
    "r = sr.Recognizer()\n",
    "mic = sr.Microphone()\n",
    "sr.Microphone.list_microphone_names()\n",
    "mic = sr.Microphone(device_index=1)\n",
    "with mic as source:\n",
    "    r.adjust_for_ambient_noise(source)\n",
    "    audio = r.listen(source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n91hmFNMPVHm"
   },
   "source": [
    "## Now, converting the audio recieved to text stored in \"translate_sentence\"; then getting model to convert the text to piezoelectric sensors' input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YguqSW2Jo7pO",
    "outputId": "70b5de31-833a-4b95-db92-9258b24046e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:/Users/ROHIT/Desktop/Hackfest/3/model/checkpoints/dev\n",
      "Input\n",
      "  Word Ids:      [2, 24, 34]\n",
      "  English Words: ['<UNK>', 'eating', 'burger']\n",
      "\n",
      "Prediction\n",
      "  Word Ids:      [25, 29, 9, 1]\n",
      "  Vibration Binary words: 01110 11001 01010 <EOS>\n"
     ]
    }
   ],
   "source": [
    "translate_sentence = r.recognize_google(audio)\n",
    "\n",
    "\n",
    "translate_sentence = sentence_to_seq(translate_sentence, source_vocab_to_int)\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(load_path + '.meta')\n",
    "    loader.restore(sess, load_path)\n",
    "\n",
    "    input_data = loaded_graph.get_tensor_by_name('input:0')\n",
    "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
    "    target_sequence_length = loaded_graph.get_tensor_by_name('target_sequence_length:0')\n",
    "    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "\n",
    "    translate_logits = sess.run(logits, {input_data: [translate_sentence]*batch_size,\n",
    "                                         target_sequence_length: [len(translate_sentence)*2]*batch_size,\n",
    "                                         keep_prob: 1.0})[0]\n",
    "\n",
    "print('Input')\n",
    "print('  Word Ids:      {}'.format([i for i in translate_sentence]))\n",
    "print('  English Words: {}'.format([source_int_to_vocab[i] for i in translate_sentence]))\n",
    "\n",
    "print('\\nPrediction')\n",
    "print('  Word Ids:      {}'.format([i for i in translate_logits]))\n",
    "print('  Vibration Binary words: {}'.format(\" \".join([target_int_to_vocab[i] for i in translate_logits])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import serial #Serial imported for Serial communication\n",
    "import time #Required to use delay functions\n",
    " \n",
    "ArduinoSerial = serial.Serial('COM3',9600) #Create Serial port object called arduinoSerialData\n",
    "time.sleep(2) #wait for 2 secounds for the communication to get established\n",
    "\n",
    "print(ArduinoSerial.readline()) #read the serial data and print it as line\n",
    "print (\"Enter 1 to turn ON Vibration Module and 0 to turn OFF Vibration Module\")\n",
    "\n",
    "l=translate_sentence.split()\n",
    " \n",
    "while 1: #Do this forever\n",
    "\n",
    "    var = input() #get input from user\n",
    "    print(\"start the Module\", var) #print the intput for confirmation\n",
    "    \n",
    "    if (var == \"1\"): #if the value is 1\n",
    "        print(\"Yes\")\n",
    "        ArduinoSerial.write(b'1') #send 1\n",
    "        print(\"Vibration turned ON\")\n",
    "        time.sleep(1)\n",
    "    \n",
    "    if (var == \"0\"): #if the value is 0\n",
    "        ArduinoSerial.write(b'0') #send 0\n",
    "        print (\"Vibration turned OF1F\")\n",
    "        time.sleep(1)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "10.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
